dataset:
  path: ./data/entex_toy.jsonl
  train_ratio: 0.8

train:
  batch_size: 4
  epochs: 10000
  model_type: lqae
  accumulate_grad_batches: 1
  dataloader_n_workers: 6
  strategy: ddp
  accelerator: gpu
  cpu_device: 1
  gpu_device: [1]
  num_nodes: 1
  precision: 32
  shuffle: true
  strategy: ddp
  gradient_clip_val: 5.0
  fast_dev_run: true
  num_workers: 6
  save_path: ./entex/checkpoint/
  resume: false
  resume_from_checkpoint: ./entex/checkpoint/epoch=3-step=4000.ckpt
  val_check_interval: 1.0
  save_monitor: validation_loss
  stop_monitor: validataion_loss
  save_every_n_step: 1000
  save_top_k: 1
  num_sanity_val_steps: 0

logging:
  log_every_n_steps: 100
  progress_bar_refresh_rate: 20
  wandb_project: entex_test
  track_grad_norm: 2.0

model:
  hidden_dim: 768
  resnet:
    hidden_dim: 512
    filters: 128
    hidden_channels: [32, 64, 128]
    channel_multipliers: [1024, 2048]
    hidden_size: 768
    conv_downsample: false
    out_channels: 7
    num_res_blocks: 3
    output_dim: 6
  bert:
    model_name: allenai/scibert_scivocab_uncased
    model_path: ./checkpoint/scibert_scivocab_uncased/
    bert_min_ratio: 0.4
    bert_max_ratio: 0.4
    use_bert_codebook: true
    bert_loss_mask_only: true
    bert_mask_loss_weight: 1.0
    use_bert_ste: true
    hidden_dim: 768
    max_position_embeddings: 1024
    hidden_dropout_prob: 0.1
  quantizer:
    quantizer_loss_entropy: 0.0
    entropy_temperature: 0.01
    entropy_loss_type: softmax
    quantizer_loss_commitment: 1.0
    l2_normalize: false
    top_k_value: 1
    top_k_avg: false
    top_k_rnd: false
    quantizer_latent_dim: 768 #dim for codebook
    strawman_codebook: false
    strawman_codebook_init: normal:1.0
    quantizer_loss_perplexity: 0.0
    dot_product: true
    quantizer_hidden_dim: 512 #dim for hidden space
  backbone:
    encode_dim: 512
  opt:
    name: opt-125M
    model_path: checkpoint/opt-125M
    max_position_embeddings: 1024
    hidden_dim: 768
    hidden_dropout_prob: 0.1
    use_opt_ste: True
    opt_autoregressive_loss: 0.25

optimizer:
  name: adam
  learning_rate: 0.0001
  weight_decay: 0.0001
  lr_scheduler: warmup_linear
  warmup_steps: 16000
  adam_betas: [0.9, 0.999]
  max_steps: 100000

seed: 42
