seed: 42
base_name: &base_name reconstruct
hidden_dim: &hidden_dim 768

dataset:
  path: data/entex_median.jsonl
  train_ratio: 0.8

train:
  batch_size: 4
  epochs: 100
  model_type: lqae
  accumulate_grad_batches: 2
  strategy: ddp
  accelerator: gpu
  cpu_device: 1
  gpu_device: [4,5,6,7]
  num_nodes: 1
  precision: 32
  shuffle: true
  strategy: ddp
  gradient_clip_val: 5.0
  fast_dev_run: false
  num_workers: 4
  save_path: 
    - checkpoint/entex/
    - *base_name
    - /
  resume: false
  resume_from_checkpoint: 
    - checkpoint/entex/
    - *base_name
    - /
    - last.ckpt
  val_check_interval: 1.0
  save_monitor: valid_loss
  stop_monitor: valid_loss
  save_every_n_step: 1000
  save_top_k: 3
  num_sanity_val_steps: 0

logging:
  log_every_n_steps: 10
  progress_bar_refresh_rate: 20
  wandb_project: 
    - entex_opt
    - _
    - *base_name
  track_grad_norm: 2.0

model:
  hidden_dim: *hidden_dim
  resnet:
    hidden_dim: 512
    filters: 128
    hidden_channels: [32, 64, 128]
    channel_multipliers: [1024, 2048]
    hidden_size: 768
    conv_downsample: false
    out_channels: 7
    num_res_blocks: 3
    output_dim: 6
  quantizer:
    quantizer_loss_entropy: 0.0
    entropy_temperature: 0.01
    entropy_loss_type: softmax
    quantizer_loss_commitment: 0.0
    l2_normalize: true
    top_k_value: 1
    top_k_avg: false
    top_k_rnd: false
    gumbel_softmax: false
    quantizer_latent_dim: 768 #dim for codebook
    strawman_codebook: false
    strawman_codebook_init: normal:1.0
    quantizer_loss_perplexity: 0.0
    dot_product: false
    quantizer_hidden_dim: 512 #dim for hidden space
  backbone:
    encode_dim: 512
  opt:
    name: opt-125M
    model_path: checkpoint/opt-125M
    max_position_embeddings: 1024
    hidden_dim: 768
    hidden_dropout_prob: 0.1
    use_opt_ste: True
    ar_loss_weight: 0.0
  decoder:
    num_layers: 3
    head_dim: *hidden_dim
    num_heads: 8
    p_drop: 0.1
    hidden_dim: *hidden_dim
    output_dim: 6

optimizer:
  name: adam
  learning_rate: 0.001
  weight_decay: 0.0001
  warmup_steps: 1000
  adam_betas: [0.9, 0.999]
  max_steps: 100000
  lr_scheduler: warmup_linear