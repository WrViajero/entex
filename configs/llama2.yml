seed: 42

dataset:
  path: data/entex_toy.jsonl
  train_ratio: 0.8

train:
  batch_size: 4
  epochs: 100
  model_type: lqae
  accumulate_grad_batches: 1
  strategy: ddp
  accelerator: gpu
  cpu_device: 1
  gpu_device: [0]
  num_nodes: 1
  precision: 32
  shuffle: false
  strategy: ddp
  gradient_clip_val: 5.0
  fast_dev_run: true
  num_workers: 4
  save_path: checkpoint/entex/opt_exp5/
  resume: false
  resume_from_checkpoint: None
  val_check_interval: 1.0
  save_monitor: valid_loss
  stop_monitor: valid_loss
  save_every_n_step: 1000
  save_top_k: 1
  num_sanity_val_steps: 0

logging:
  log_every_n_steps: 10
  progress_bar_refresh_rate: 20
  wandb_project: entex_opt_exp5
  track_grad_norm: 2.0

model:
  hidden_dim: 768
  resnet:
    hidden_dim: 512
    filters: 128
    hidden_channels: [32, 64, 128]
    channel_multipliers: [1024, 2048]
    hidden_size: 768
    conv_downsample: false
    out_channels: 7
    num_res_blocks: 3
    output_dim: 6
  quantizer:
    quantizer_loss_entropy: 0.0
    entropy_temperature: 0.01
    entropy_loss_type: softmax
    quantizer_loss_commitment: 1.0
    l2_normalize: false
    top_k_value: 1
    top_k_avg: false
    top_k_rnd: false
    quantizer_latent_dim: 768 #dim for codebook
    strawman_codebook: false
    strawman_codebook_init: normal:1.0
    quantizer_loss_perplexity: 0.0
    dot_product: true
    quantizer_hidden_dim: 512 #dim for hidden space
  backbone:
    encode_dim: 512
  llama2:
    name: llama2
    model_path: checkpoint/llama2
    max_position_embeddings: 1024
    hidden_dim: 768
    hidden_dropout_prob: 0.1
    use_opt_ste: True
    opt_autoregressive_loss: 0.25

optimizer:
  name: adam
  learning_rate: 0.00001
  weight_decay: 0.0001
  warmup_steps: 1000
  adam_betas: [0.9, 0.999]
  max_steps: 100000
  lr_scheduler: warmup_linear